# -*- coding: utf-8 -*-
"""Neural Network A Simple Perception - Assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tyWtbt0nvnzUiSN00T_GSLdQegGpNleW

# **Neural Network A Simple Perception**

1. What is deep learning, and how is it connected to artificial intelligence?
- Deep learning is a subset of machine learning, which itself falls under the broader field of artificial intelligence (AI). Artificial intelligence refers to the development of computer systems that can perform tasks typically requiring human intelligence, such as reasoning, problem-solving, understanding language, and recognizing patterns.
- Within AI, machine learning focuses on enabling machines to learn from data and improve their performance over time without being explicitly programmed. Deep learning takes this a step further by using complex structures called artificial neural networks, which are inspired by the human brain. These networks consist of multiple layers that allow the system to learn increasingly abstract features from large amounts of data.
- Deep learning is particularly powerful for tasks such as image recognition, natural language processing, and autonomous driving. In essence, deep learning is a specialized and advanced technique that drives many of the most impressive and cutting-edge applications of AI today.

2. What is a neural network, and what are the different types of neural networks?
- A neural network is a computational model inspired by the structure and function of the human brain. It consists of layers of interconnected units called neurons, which process and transmit data. Each neuron receives input, applies a mathematical transformation (usually involving weights, biases, and an activation function), and passes the output to the next layer. Neural networks are the foundation of deep learning, allowing machines to learn complex patterns and make intelligent decisions from data. There are several types of neural networks, each designed for specific tasks:
     1. Feedforward Neural Network (FNN)
     2. Convolutional Neural Network (CNN)
     3. Recurrent Neural Network (RNN)
     4. Long Short-Term Memory (LSTM) and GRU Networks
     5. Autoencoders
     6. Generative Adversarial Networks (GANs)
     7. Radial Basis Function (RBF) Networks
     8. Modular Neural Networks

3. What is the mathematical structure of a neural network?
- A neural network is a mathematical model composed of layers of interconnected nodes (neurons), where each layer transforms its input through a linear combination followed by a non-linear activation function. Mathematically, each neuron computes an output of the form
𝑎
=
𝜙
(
𝑤
⊤
𝑥 +
𝑏
)
a=ϕ(w
⊤
 x+b), where
𝑥
x is the input vector,
𝑤
w is the weight vector,
𝑏
b is a bias term, and
𝜙
ϕ is a non-linear activation function such as ReLU, sigmoid, or tanh. Layers are stacked such that the output of one becomes the input of the next, resulting in a nested composition of functions.
- The network as a whole can be represented as
𝑓
(
𝑥
;
𝜃
)
=
𝑓
(
𝐿
)
∘
𝑓
(
𝐿
−
1
)
∘
⋯
∘
𝑓
(
1
)
(
𝑥
)
f(x;θ)=f
(L)
 ∘f
(L−1)
 ∘⋯∘f
(1)
 (x), where
θ denotes all the weights and biases. Training the network involves adjusting these parameters to minimize a loss function using optimization algorithms like gradient descent, with gradients computed via backpropagation. This structure allows neural networks to approximate highly complex and non-linear mappings from inputs to outputs.

4. What is an activation function, and why is it essential in neural network ?
- An activation function is a mathematical function applied to the output of each neuron in a neural network, introducing non-linearity into the model. It determines whether a neuron should be activated or not by transforming the weighted sum of inputs into a non-linear output. This non-linearity is essential because, without it, a neural network composed only of linear operations would behave like a single-layer linear model, regardless of how many layers it has, and thus would be unable to capture complex patterns in data.
- Activation functions like ReLU (Rectified Linear Unit), sigmoid, and tanh enable the network to learn and approximate intricate relationships, making them crucial for solving problems in classification, regression, image processing, and more. By allowing neural networks to model non-linear decision boundaries, activation functions significantly enhance the expressive power and flexibility of deep learning models.

5. Could you list some common activation functions used in neural networks ?
- Here are the some common activation functions that are used in the neural networks:-
    1. ReLU (Rectified Linear Unit)
    2. Sigmoid
    3. Tanh (Hyperbolic Tangent)
    4. Leaky ReLU
    5. Softmax
    6. ELU (Exponential Linear Unit)

6. What is a multilayer neural network ?
- A multilayer neural network, also known as a multilayer perceptron (MLP), is a type of artificial neural network that consists of an input layer, one or more hidden layers, and an output layer. Each layer is made up of neurons, and each neuron in a given layer is connected to every neuron in the next layer through weighted connections.
- The network processes input data by passing it through these layers, where each neuron performs a weighted sum of its inputs followed by the application of an activation function. The presence of multiple hidden layers allows the network to learn complex, non-linear relationships in the data, making it capable of solving tasks such as image recognition, language processing, and prediction. Multilayer neural networks are trained using algorithms like backpropagation, which adjust the weights to minimize the error between the predicted output and the actual target. The depth and structure of these networks play a crucial role in their ability to model sophisticated patterns and perform accurately on a wide range of tasks.

7. What is a loss function, and why is it crucial for neural network training ?
- A loss function measures how far the neural network’s predictions are from the true answers. It tells the network how well it is doing. During training, the network tries to make this loss as small as possible by adjusting its weights. Without a loss function, the network wouldn’t know if it’s improving or not. Different loss functions are used for different tasks, like predicting numbers or classifying categories. Overall, the loss function is essential because it guides the network to learn and get better at its task.

8. What are some common types of loss functions?
- Here are some common types of loss functions used in neural networks:
   1. Mean Squared Error (MSE)
   2. Mean Absolute Error (MAE)
   3. Cross-Entropy Loss (Log Loss)
   4. Hinge Loss
   5. Huber Loss

9. How does a neural network learn?
- A neural network learns by adjusting its internal parameters (weights and biases) to make better predictions. It starts by making a guess based on random weights, then compares its output to the correct answer using a loss function. The difference (loss) shows how wrong the prediction is.
- Using this loss, the network calculates gradients—essentially figuring out how to change each weight to reduce the error. This process is done through backpropagation and optimization algorithms like gradient descent, which update the weights step-by-step. By repeating this many times on training data, the network gradually improves and learns to make accurate predictions.

10. What is an optimizer in neural networks, and why is it necessary ?
- An optimizer in neural networks is an algorithm that updates the network’s weights and biases to minimize the loss function during training. It decides how much and in which direction the parameters should change based on the gradients calculated through backpropagation.
-  Optimizers are necessary because they guide the learning process, helping the network improve its performance by efficiently finding the best set of weights. Without an optimizer, the network wouldn’t know how to adjust its parameters to reduce errors, making training impossible. Popular optimizers include Gradient Descent, Adam, and RMSprop, each with different ways to speed up and stabilize learning.

11. Could you briefly describe some common optimizers.
- Here are some common optimizers used in neural networks:
    1. Gradient Descent (GD)
    2. Stochastic Gradient Descent (SGD)
    3. Mini-batch Gradient Descent
    4. Adam (Adaptive Moment Estimation)
    5. RMSprop

12. Can you explain forward and backward propagation in a neural network ?
- Forward Propagation = Forward propagation is the process where the input data passes through the network layer by layer. At each neuron, the network calculates a weighted sum of the inputs, adds a bias, and applies an activation function to produce an output. This continues through all layers until the network produces its final prediction or output.
- Backward Propagation = Backward propagation (or backpropagation) happens after forward propagation. It’s how the network learns from its mistakes. The network compares its output to the true target using a loss function, which measures the error. Then, it calculates how much each weight in the network contributed to this error by computing gradients using the chain rule of calculus. These gradients are sent backward through the network, layer by layer, allowing the optimizer to update the weights to reduce the error. By repeating forward and backward passes many times, the network gradually improves its predictions.

13. What is weight initialization, and how does it impact training ?
- Weight initialization is the process of setting the starting values of a neural network’s weights before training begins. Proper initialization is important because it affects how quickly and effectively the network learns. If weights are initialized too large, the activations can explode and cause unstable training; if too small, they can vanish and slow down learning. Good initialization helps maintain balanced signals as they pass through layers, avoiding problems like vanishing or exploding gradients.
- Techniques like Xavier (Glorot) initialization and He initialization are commonly used to set weights in a way that keeps training stable and speeds up convergence. In short, weight initialization sets the stage for efficient and successful training of neural networks.

14. What is the vanishing gradient problem in deep learning ?
- The vanishing gradient problem occurs in deep learning when the gradients used to update a neural network’s weights become extremely small as they are propagated backward through many layers. This makes the earlier layers learn very slowly or stop learning altogether because their weight updates become tiny.
- It usually happens with certain activation functions like sigmoid or tanh, which squash values into small ranges, causing gradients to shrink during backpropagation. As a result, deep networks struggle to capture important features in early layers, limiting their ability to learn complex patterns. Techniques like using ReLU activation, proper weight initialization, and specialized architectures help reduce this problem.

15. What is the exploding gradient problem?
- The exploding gradient problem happens when the gradients used to update a neural network’s weights become excessively large during backpropagation. This causes huge updates to the weights, making the training process unstable and often leading to numerical overflow or the model failing to converge.
- It usually occurs in very deep networks or recurrent neural networks when gradients accumulate and grow exponentially as they are propagated backward through many layers. To fix this, techniques like gradient clipping (limiting the size of gradients) and careful weight initialization are used to keep training stable and prevent the gradients from becoming too large.

# **Practical**
"""

# Q1. How do you create a simple perceptron for basic binary classification?


import numpy as np

class Perceptron:
    def __init__(self, input_size, learning_rate=0.1, epochs=100):
        self.weights = np.zeros(input_size)
        self.bias = 0
        self.learning_rate = learning_rate
        self.epochs = epochs

    def activation(self, x):
        return 1 if x >= 0 else 0

    def predict(self, x):
        z = np.dot(self.weights, x) + self.bias
        return self.activation(z)

    def fit(self, X, y):
        for _ in range(self.epochs):
            for xi, target in zip(X, y):
                prediction = self.predict(xi)
                error = target - prediction
                self.weights += self.learning_rate * error * xi
                self.bias += self.learning_rate * error

if __name__ == "__main__":
    X = np.array([
        [0, 0],
        [0, 1],
        [1, 0],
        [1, 1]
    ])
    y = np.array([0, 1, 1, 1])

    perceptron = Perceptron(input_size=2)
    perceptron.fit(X, y)

    for point in X:
        print(f"Input: {point}, Predicted Output: {perceptron.predict(point)}")

# Q2. How can you build a neural network with one hidden layer using Keras ?

import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Input

input_dim = 10

X_train = np.random.rand(100, input_dim)
y_train = np.random.randint(0, 2, 100)

X_test = np.random.rand(20, input_dim)
y_test = np.random.randint(0, 2, 20)

model = Sequential([
    Input(shape=(input_dim,)),
    Dense(8, activation='relu'),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

history = model.fit(X_train, y_train, epochs=10, batch_size=16)

loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test loss: {loss:.4f}, Test accuracy: {accuracy:.4f}")

predictions = model.predict(X_test)
print("Predictions:\n", predictions)

# Q3. How do you initialize weights using the Xavier (Glorot) initialization method in Keras ?


import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Input

input_dim = 10

X_train = np.random.rand(100, input_dim)
y_train = np.random.randint(0, 2, size=(100,))

X_test = np.random.rand(20, input_dim)
y_test = np.random.randint(0, 2, size=(20,))

model = Sequential([
    Input(shape=(input_dim,)),
    Dense(8, activation='relu', kernel_initializer='glorot_uniform'),
    Dense(1, activation='sigmoid', kernel_initializer='glorot_uniform')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

history = model.fit(X_train, y_train, epochs=10, batch_size=16, verbose=1)

loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f"\nTest Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}")

predictions = model.predict(X_test)
print("\nPredictions:")
print(predictions)

# Q4.  How can you apply different activation functions in a neural network in Keras ?

import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Input

input_dim = 10
X_train = np.random.rand(100, input_dim)
y_train = np.random.randint(0, 2, size=(100,))
X_test = np.random.rand(20, input_dim)
y_test = np.random.randint(0, 2, size=(20,))

model = Sequential([
    Input(shape=(input_dim,)),
    Dense(16, activation='relu'),
    Dense(8, activation='tanh'),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

print("Training...")
model.fit(X_train, y_train, epochs=10, batch_size=16, verbose=1)

loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f"\nTest Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}")

predictions = model.predict(X_test)
print("\nPredictions (probabilities):")
print(predictions)

# Q5. How do you add dropout to a neural network model to prevent overfitting ?


from tensorflow.keras.models import Sequential
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Input

input_dim = 10
X_train = np.random.rand(100, input_dim)
y_train = np.random.randint(0, 2, 100)

X_test = np.random.rand(20, input_dim)
y_test = np.random.randint(0, 2, 20)

model = Sequential([
    Input(shape=(input_dim,)),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(32, activation='relu'),
    Dropout(0.3),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

print("Training...")
model.fit(X_train, y_train, epochs=10, batch_size=16, verbose=1)

loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f"\nTest Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}")

predictions = model.predict(X_test)
print("\nPredictions (probabilities):")
print(predictions)

# Q6. How do you manually implement forward propagation in a simple neural network.

import numpy as np

X = np.array([[0.5, 0.2, 0.1]])

W1 = np.array([[0.1, 0.3, 0.5, 0.2],
               [0.2, 0.4, 0.1, 0.3],
               [0.4, 0.6, 0.3, 0.7]])
b1 = np.array([[0.01, 0.02, 0.03, 0.04]])

def relu(x):
    return np.maximum(0, x)

Z1 = np.dot(X, W1) + b1
A1 = relu(Z1)

W2 = np.array([[0.5], [0.3], [0.2], [0.6]])
b2 = np.array([[0.1]])

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

Z2 = np.dot(A1, W2) + b2
A2 = sigmoid(Z2)

print("Output:", A2)

# Q7. How do you add batch normalization to a neural network model in Keras.


import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Input
from tensorflow.keras.optimizers import Adam

input_dim = 10
X_train = np.random.rand(100, input_dim)
y_train = np.random.randint(0, 2, size=(100,))

X_test = np.random.rand(20, input_dim)
y_test = np.random.randint(0, 2, size=(20,))

model = Sequential([
    Input(shape=(input_dim,)),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(32, activation='relu'),
    Dropout(0.3),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])

print("Training the model...")
model.fit(X_train, y_train, epochs=10, batch_size=16, verbose=1)

loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f"\n✅ Test Loss: {loss:.4f}")
print(f"✅ Test Accuracy: {accuracy:.4f}")

predictions = model.predict(X_test)
print("\n🔮 Predictions (probabilities):")
print(predictions.flatten())

# Q8. How can you visualize the training process with accuracy and loss curves ?


import matplotlib.pyplot as plt

epochs = range(1, 11)
train_accuracy = [0.6, 0.7, 0.75, 0.78, 0.8, 0.82, 0.85, 0.87, 0.88, 0.9]
val_accuracy = [0.58, 0.68, 0.7, 0.74, 0.76, 0.77, 0.8, 0.82, 0.83, 0.85]
train_loss = [1.2, 0.9, 0.7, 0.55, 0.48, 0.42, 0.36, 0.31, 0.29, 0.25]
val_loss = [1.3, 1.0, 0.85, 0.65, 0.6, 0.55, 0.5, 0.45, 0.43, 0.4]

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(epochs, train_accuracy, 'bo-', label='Training Accuracy')
plt.plot(epochs, val_accuracy, 'ro-', label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(epochs, train_loss, 'bo-', label='Training Loss')
plt.plot(epochs, val_loss, 'ro-', label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

# Q9. How can you use gradient clipping in Keras to control the gradient size and prevent exploding gradients.


import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0
x_train = x_train.reshape(-1, 28*28)
x_test = x_test.reshape(-1, 28*28)

y_train = tf.keras.utils.to_categorical(y_train, 10)
y_test = tf.keras.utils.to_categorical(y_test, 10)

model = models.Sequential([
    layers.Dense(128, activation='relu', input_shape=(28*28,)),
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')
])

optimizer = Adam(learning_rate=0.001, clipnorm=1.0)

model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=5, batch_size=64)

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], 'bo-', label='Train Accuracy')
plt.plot(history.history['val_accuracy'], 'ro-', label='Validation Accuracy')
plt.title('Accuracy with Gradient Clipping')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], 'bo-', label='Train Loss')
plt.plot(history.history['val_loss'], 'ro-', label='Validation Loss')
plt.title('Loss with Gradient Clipping')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

# Q10. How can you create a custom loss function in Keras.


import tensorflow as tf
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0
x_train = x_train.reshape(-1, 28*28)
x_test = x_test.reshape(-1, 28*28)

y_train = tf.keras.utils.to_categorical(y_train, 10)
y_test = tf.keras.utils.to_categorical(y_test, 10)

def custom_loss(y_true, y_pred):
    mse = tf.reduce_mean(tf.square(y_pred - y_true))
    penalty = 0.1 * tf.reduce_sum(tf.abs(y_pred))
    return mse + penalty

model = models.Sequential([
    layers.Dense(128, activation='relu', input_shape=(28*28,)),
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam', loss=custom_loss, metrics=['accuracy'])

history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=5, batch_size=64)

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], 'bo-', label='Train Accuracy')
plt.plot(history.history['val_accuracy'], 'ro-', label='Validation Accuracy')
plt.title('Accuracy with Custom Loss')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], 'bo-', label='Train Loss')
plt.plot(history.history['val_loss'], 'ro-', label='Validation Loss')
plt.title('Loss with Custom Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

# Q11. How can you visualize the structure of a neural network model in Keras?


import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.utils import plot_model
from IPython.display import Image

model = models.Sequential([
    layers.Dense(64, activation='relu', input_shape=(100,)),
    layers.Dense(10, activation='softmax')
])

print("Model Summary:")
model.summary()

plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

Image('model_plot.png')

"""# Thank You Sir/Ma'am,
**Your's Sincerly,
Darshan Panchal**
"""